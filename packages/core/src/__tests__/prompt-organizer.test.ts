import { describe, it, expect, beforeEach, afterEach, vi } from 'vitest';
import { PromptOrganizer } from '../prompt-organizer.js';
import type { LLMConfig, Prompt, Category } from '../models.js';

const mockConfig: LLMConfig = {
  baseUrl: 'https://fake-llm.test/v1',
  apiKey: 'sk-test-key',
  model: 'test-model',
};

const mockFetch = vi.fn();

function makeMockPrompt(overrides: Partial<Prompt> & { id: string }): Prompt {
  return {
    title: 'Test Prompt',
    content: 'Some test content for the prompt',
    categoryId: 'cat-1',
    tags: ['test'],
    isFavorite: false,
    usageCount: 0,
    createdAt: '2026-01-01T00:00:00.000Z',
    updatedAt: '2026-01-01T00:00:00.000Z',
    ...overrides,
  };
}

const mockCategories: Category[] = [
  { id: 'cat-1', name: 'ç¼–ç¨‹', icon: 'ðŸ’»', sortOrder: 0 },
  { id: 'cat-2', name: 'å†™ä½œ', icon: 'âœï¸', sortOrder: 1 },
  { id: 'cat-3', name: 'å…¶ä»–', icon: 'ðŸ“', sortOrder: 99 },
];

describe('PromptOrganizer', () => {
  let organizer: PromptOrganizer;

  beforeEach(() => {
    vi.stubGlobal('fetch', mockFetch);
    organizer = new PromptOrganizer(mockConfig);
  });

  afterEach(() => {
    vi.restoreAllMocks();
  });

  function mockLLMResponse(content: string) {
    mockFetch.mockResolvedValueOnce({
      ok: true,
      json: async () => ({
        choices: [{ message: { content } }],
      }),
    });
  }

  describe('scan', () => {
    it('should return suggestions for prompts that need optimization', async () => {
      const prompts = [
        makeMockPrompt({ id: 'p1', title: 'Pythonä»£ç ...', content: 'å¸®æˆ‘å†™ä¸€ä¸ªPythonå¼‚å¸¸å¤„ç†çš„æœ€ä½³å®žè·µæç¤ºè¯', categoryId: 'cat-3', tags: [] }),
        makeMockPrompt({ id: 'p2', title: 'å¥½ç”¨çš„ç¿»è¯‘', content: 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¸­è‹±æ–‡ç¿»è¯‘åŠ©æ‰‹', categoryId: 'cat-1', tags: ['ç¿»è¯‘'] }),
      ];

      const llmResponse = JSON.stringify({
        suggestions: [
          {
            promptId: 'p1', newTitle: 'Pythonå¼‚å¸¸å¤„ç†æ¨¡æ¿', newCategory: 'ç¼–ç¨‹',
            isNewCategory: false, newTags: ['python', 'å¼‚å¸¸å¤„ç†'], similarTo: [],
            reason: 'æ ‡é¢˜æ¨¡ç³Šï¼Œåˆ†ç±»åº”ä¸ºç¼–ç¨‹',
          },
          {
            promptId: 'p2', newTitle: 'ä¸­è‹±æ–‡ç¿»è¯‘åŠ©æ‰‹', newCategory: 'ç¿»è¯‘',
            isNewCategory: false, newTags: ['ç¿»è¯‘', 'ä¸­è‹±æ–‡'], similarTo: [],
            reason: 'åˆ†ç±»åº”ä¸ºç¿»è¯‘è€Œéžç¼–ç¨‹',
          },
        ],
      });
      mockLLMResponse(llmResponse);

      const result = await organizer.scan(prompts, mockCategories);
      expect(result.suggestions.length).toBe(2);
      expect(result.totalScanned).toBe(2);
      expect(result.batchesCompleted).toBe(1);
      expect(result.batchesFailed).toBe(0);

      const s1 = result.suggestions.find(s => s.promptId === 'p1')!;
      expect(s1.newTitle).toBe('Pythonå¼‚å¸¸å¤„ç†æ¨¡æ¿');
      expect(s1.newCategory).toBe('ç¼–ç¨‹');
      expect(s1.newTags).toEqual(['python', 'å¼‚å¸¸å¤„ç†']);
    });

    it('should batch prompts when there are many', async () => {
      const prompts = Array.from({ length: 25 }, (_, i) =>
        makeMockPrompt({ id: `p${i}`, title: `Prompt ${i}`, content: `Content ${i}` })
      );

      const batch1Response = JSON.stringify({
        suggestions: prompts.slice(0, 15).map(p => ({
          promptId: p.id, newTitle: null, newCategory: null,
          isNewCategory: false, newTags: null, similarTo: [], reason: 'æ— éœ€ä¿®æ”¹',
        })),
      });
      const batch2Response = JSON.stringify({
        suggestions: prompts.slice(15).map(p => ({
          promptId: p.id, newTitle: null, newCategory: null,
          isNewCategory: false, newTags: null, similarTo: [], reason: 'æ— éœ€ä¿®æ”¹',
        })),
      });
      mockLLMResponse(batch1Response);
      mockLLMResponse(batch2Response);

      const result = await organizer.scan(prompts, mockCategories);
      expect(mockFetch).toHaveBeenCalledTimes(2);
      expect(result.totalScanned).toBe(25);
      expect(result.batchesCompleted).toBe(2);
    });

    it('should handle LLM errors gracefully by skipping failed batches', async () => {
      const prompts = Array.from({ length: 25 }, (_, i) =>
        makeMockPrompt({ id: `p${i}`, title: `Prompt ${i}`, content: `Content ${i}` })
      );

      const batch1Response = JSON.stringify({
        suggestions: prompts.slice(0, 15).map(p => ({
          promptId: p.id, newTitle: null, newCategory: null,
          isNewCategory: false, newTags: null, similarTo: [], reason: 'æ— éœ€ä¿®æ”¹',
        })),
      });
      mockLLMResponse(batch1Response);
      mockFetch.mockResolvedValueOnce({
        ok: false, status: 500, text: async () => 'Internal Server Error',
      });

      const result = await organizer.scan(prompts, mockCategories);
      expect(result.batchesCompleted).toBe(1);
      expect(result.batchesFailed).toBe(1);
      expect(result.suggestions.length).toBe(15);
    });

    it('should send correct system prompt with categories context', async () => {
      const prompts = [
        makeMockPrompt({ id: 'p1', title: 'Test', content: 'Test content', tags: ['existing-tag'] }),
      ];

      const llmResponse = JSON.stringify({
        suggestions: [{
          promptId: 'p1', newTitle: null, newCategory: null,
          isNewCategory: false, newTags: null, similarTo: [], reason: 'æ— éœ€ä¿®æ”¹',
        }],
      });
      mockLLMResponse(llmResponse);

      await organizer.scan(prompts, mockCategories);

      const callBody = JSON.parse(mockFetch.mock.calls[0][1].body);
      const systemMsg = callBody.messages[0].content;
      expect(systemMsg).toContain('ç¼–ç¨‹');
      expect(systemMsg).toContain('å†™ä½œ');
      expect(systemMsg).toContain('å…¶ä»–');
    });

    it('should truncate prompt content to 500 chars', async () => {
      const longContent = 'A'.repeat(1000);
      const prompts = [makeMockPrompt({ id: 'p1', title: 'Long', content: longContent })];

      const llmResponse = JSON.stringify({
        suggestions: [{
          promptId: 'p1', newTitle: null, newCategory: null,
          isNewCategory: false, newTags: null, similarTo: [], reason: '',
        }],
      });
      mockLLMResponse(llmResponse);

      await organizer.scan(prompts, mockCategories);

      const callBody = JSON.parse(mockFetch.mock.calls[0][1].body);
      const userMsg = callBody.messages[1].content;
      expect(userMsg.length).toBeLessThan(longContent.length + 500);
    });

    it('should return empty results for empty prompt list', async () => {
      const result = await organizer.scan([], mockCategories);
      expect(result.suggestions).toEqual([]);
      expect(result.totalScanned).toBe(0);
      expect(mockFetch).not.toHaveBeenCalled();
    });

    it('should detect duplicates via similarTo field', async () => {
      const prompts = [
        makeMockPrompt({ id: 'p1', title: 'Pythonå¸®åŠ©', content: 'å¸®æˆ‘å†™Pythonä»£ç ' }),
        makeMockPrompt({ id: 'p2', title: 'PythonåŠ©æ‰‹', content: 'å¸®æˆ‘ç¼–å†™Pythonä»£ç ' }),
      ];

      const llmResponse = JSON.stringify({
        suggestions: [
          { promptId: 'p1', newTitle: 'Pythonç¼–ç åŠ©æ‰‹', newCategory: null,
            isNewCategory: false, newTags: null, similarTo: ['p2'], reason: 'ä¸Žp2å†…å®¹ç›¸ä¼¼' },
          { promptId: 'p2', newTitle: 'Pythonç¼–ç åŠ©æ‰‹', newCategory: null,
            isNewCategory: false, newTags: null, similarTo: ['p1'], reason: 'ä¸Žp1å†…å®¹ç›¸ä¼¼' },
        ],
      });
      mockLLMResponse(llmResponse);

      const result = await organizer.scan(prompts, mockCategories);
      const s1 = result.suggestions.find(s => s.promptId === 'p1')!;
      expect(s1.similarTo).toContain('p2');
    });
  });
});
